# TP1-Aprendizaje-profundo-Red-Hopfield
Peque√±a pero breve introducci√≥n: Este proyecto implementa una red neuronal de Hopfield, un tipo de red recurrente que se utiliza como memoria asociativa. B√°sicamente, la red puede ‚Äòrecordar‚Äô patrones a partir de ejemplos, incluso cuando los recibe incompletos o con ruido.

![Enunciado TP 1](imagen_2025-09-06_014827567.png)

Resoluci√≥n:
## 1 - a) Verificaci√≥n del aprendizaje. La red recuerda lo que aprendi√≥?
En esta secci√≥n, cargamos 3 im√°genes simples (paloma, perro y panda):

![Imagenes](imagen_2025-09-06_023525787.png)

A cada imagen la transformamos en blanco y negro y la convertimos a un formato que la red puede procesar (una secuencia de valores -1 y 1, en lugar de pixeles comunes).

Luego, entrenamos la red de Hopfield con estas im√°genes: B√°sicamente construimos una matriz de conexiones entre las neuronas que guarda la ‚Äúmemoria‚Äù de cada patr√≥n.

La prueba fue la siguiente:

* Le volvimos a mostrar a la red exactamente las im√°genes que hab√≠a aprendido.
  
* Si la red hab√≠a memorizado correctamente, al procesarlas no deb√≠a modificarlas.
  
* En otras palabras, deb√≠a devolver la misma imagen que se le dio como entrada.
  
Resultado: Todas las im√°genes fueron recordadas correctamente: la red devolvi√≥ la misma que se le hab√≠a ense√±ado.
Esto significa que la red realmente aprendi√≥ esos patrones y puede almacenarlos en su ‚Äúmemoria asociativa‚Äù.

## 1 - b) Evaluaci√≥n de la red con im√°genes alteradas

En este apartado probamos qu√© tan robusta es la red de Hopfield cuando le damos im√°genes modificadas en lugar de las originales. La idea es ver si la red puede "recordar" y reconstruir correctamente la versi√≥n aprendida, incluso cuando la entrada tiene errores o est√° incompleta.
Realic√© tres tipos de pruebas:
### 1. Im√°genes parcialmente tapadas:
  * Tapamos la mitad de abajo de las 3 respectivas im√°genes.

![Imagenes](imagen_2025-09-06_231030043.png)
  * Al reconstruirlas, la red fue capaz de recuperar bastante bien la forma original, demostrando que ‚Äúrellena‚Äù la informaci√≥n faltante a partir de lo que aprendi√≥. Excepto por la √∫ltima imagen:

![Imagenes](imagen_2025-09-06_231714290.png)
![Imagenes](imagen_2025-09-06_232722530.png)

La conclusi√≥n de este punto es que incluso con un 50% de los p√≠xeles tapados, mas de la mitad de las im√°genes segu√≠an siendo reconocibles.

### 2. Im√°genes con ruido:
  * Introdujimos ruido aleatorio, invirtiendo valores de algunos p√≠xeles.
  * Con niveles bajos de ruido (10% o 30%), la red reconstruy√≥ casi perfectamente las im√°genes.
  * Con ruido muy alto (50% o 90%), los resultados fueron mucho peores, porque la entrada ya no se parece a lo aprendido.
    
  Im√°genes con 10% de ruido:
  
![Imagenes](imagenes_con_10_de_ruido.png)

  Este es el resultado de la reconstrucci√≥n que le aplic√≥ la red a las imagenes que ten√≠an 10% de ruido:

![Imagenes](imagenes_reconstruidas_por_la_red_con_10_de_ruido.png)

  Im√°genes con 50% de ruido:

![Imagenes](imagenes_con_50_de_ruido.png)

  Este es el resultado de la reconstrucci√≥n que le aplic√≥ la red a las im√°genes que ten√≠an 50% de ruido:

![Imagenes](imagenes_reconstruidas_por_la_red_con_50_de_ruido.png)

  Im√°genes con 90% de ruido:

![Imagenes](imagenes_con_90_de_ruido.png)

  Este es el resultado de la reconstrucci√≥n que le aplic√≥ la red a las im√°genes que teni√≠an 90% de ruido:

![Imagenes](imagenes_reconstruidas_por_la_red_con_90_de_ruido.jpg)

  Esto muestra el l√≠mite de memoria de la red: puede corregir errores hasta cierto punto, pero no recuperar una imagen que est√° casi destruida.

  ### 3. Im√°genes con eliminaci√≥n de p√≠xeles:
  
  * Para este caso, hemos probado con borrar los pixeles del set imagenes. Primero con el 50% de pixeles borrados y luego con el 90% de pixeles borrados.

    Pixeles borrados al 50%

![Imagenes](imagenes_con_50_de_pixeles_borrados.png)

  Pixeles borrados al 90%

![Imagenes](imagenes_con_90_de_pixeles_borrados.png)

  * La conclusi√≥n para ambos casos, es que la red fue capaz de recuperar todas las imagenes, lo que demuestra su alta capacidad de memoria para estos casos.

    Set de im√°genes recuperadas para ambos casos

![Imagenes](imagenes_recuperadas_con_50_de_pixeles_borrados.png)

![Imagenes](imagenes_recuperadas_con_90_de_pixeles_borrados.png)

## 1 - c) Estados Espurios en la Red de Hopfield
Una red de Hopfield no solo puede recuperar los patrones que le ense√±amos, sino tambi√©n otros estados no deseados llamados estados espurios. 
Estos aparecen porque, al superponerse los recuerdos, la red genera "m√≠nimos de energ√≠a falsos" que no corresponden a im√°genes originales.

* Patrones inversos: si la red aprendi√≥ una imagen con fondo blanco y figura negra, el inverso es la misma figura pero con colores invertidos. El experimento mostr√≥ que estos inversos fueron reconocidos como estados estables, por lo tanto, son estados espurios.
* Combinaciones de un n√∫mero impar de patrones: al mezclar varias im√°genes y d√°rselas a la red, esta tambi√©n las estabiliz√≥ en un estado que no era ninguna de las im√°genes originales, confirmando otro caso de estado espurio.

Primer tipo de estado espurio (Patrones inversos):

![Imagenes](estados_espurios_1.png)

Segundo tipo de estado espurio (Combinaci√≥n de n√∫mero impar de patrones):

![Imagenes](estados_espurios_2.png)

## 1 - d) Entrenamiento con las 6 im√°genes
En este punto entrenamos la red de Hopfield con las 6 im√°genes disponibles (paloma, panda, perro, quijote, torero y V de Vendetta).
Resultados: 
* La red logr√≥ recordar correctamente 4 im√°genes.
* Las im√°genes de la paloma y el panda no fueron aprendidas de forma correcta, mostrando versiones incompletas o distorsionadas.
  
Las im√°genes que utilizadas para el entrenamiento:

![Imagenes](imagenes_para_entrenar_parte1.png)

![Imagenes](imagenes_para_entrenar_parte2.png)

Las im√°genes que no fueron aprendidas:

![Imagenes](resultados.png)

Explicaci√≥n sencilla de lo que pasa:
La red de Hopfield tiene una capacidad limitada de memoria.
En teor√≠a, la cantidad m√°xima de patrones que puede almacenar sin errores est√° dada por:
  pmax‚Äã‚âà0.138√óN
donde N es la cantidad de neuronas (en este caso, los p√≠xeles de cada imagen).
Para nuestras im√°genes (52√ó52 = 2704 p√≠xeles), la capacidad m√°xima te√≥rica es:

ùëù
ùëö
ùëé
ùë•
‚âà
0.138
√ó
2704
‚âà
373
¬†patrones.

Esto es much√≠simo m√°s que 6, por lo que parecer√≠a que no deber√≠a haber problema.
Sin embargo, en la pr√°ctica la red no funciona bien cuando los patrones son similares entre s√≠ (no ortogonales). 
En nuestro caso, la paloma y el panda comparten ciertas caracter√≠sticas, lo que genera interferencia al almacenarse junto con las dem√°s im√°genes.

Conclusi√≥n: Aunque te√≥ricamente la red puede almacenar cientos de patrones, en la pr√°ctica su desempe√±o real es mucho menor debido a la superposici√≥n y similitud entre im√°genes. 
Por eso, solo 4 de las 6 fueron recordadas con √©xito.
